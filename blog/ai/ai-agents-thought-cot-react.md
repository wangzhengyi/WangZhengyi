# 从「思考」到 ReAct：AI 智能体是怎么一步步想清楚再动手的？

如果你最近在折腾 AI 智能体（AI Agents），你大概率已经见过几个关键词：
- Thought（思考） / 内部推理
- Chain-of-Thought（CoT，思维链）
- ReAct（Reasoning + Acting）

它们听起来有点学术，但背后其实在解决一个非常现实的问题：

如何让一个大模型，不只是“直接给答案”，而是像人一样：想一想 → 查一查 → 再决定下一步？

这篇文章基于 Hugging Face Agents Course 的相关章节，带你把这几个概念串起来，重点回答三个问题：
1. Thought 到底是什么？和普通“回复”有啥区别？
2. CoT 和 ReAct 分别适合什么任务？
3. 新一代“会思考的模型”（比如 DeepSeek R1、OpenAI o1）和这些提示技术是什么关系？

---

## 一、Thought：LLM 的“内心独白”，不是给用户看的那一部分

### 1. 什么是 Thought？

在智能体语境下：

Thought = 智能体在每一步行动前的内部推理与规划过程，
本质上是 LLM 的“内心独白”，通常不会直接展示给用户。

它的典型特征：
- 只在模型内部和系统之间传递；
- 用来整理当前信息、拆解任务、决定下一步行动；
- 可能被记录到日志里，方便调试/审计。

### 2. Thought 在智能体里的作用

可以把 Thought 当成智能体的“前额叶”，主要干三件事：
- 评估当前状态：
  “目前我掌握了哪些信息？哪些还缺？”
- 拆解复杂任务：
  “要完成这个目标，大概要分几步走，每一步做什么？”
- 做决策和调整计划：
  “刚才这条路走不通，要不要换个策略/工具？”

### 3. 常见的“思考类型”

下面这张表可以帮你把 Thought 的形态具体化——它不只是“我在想”，而是有明确功能的思维模式：

| 思维类型 | 示例（中文） |
| --- | --- |
| 规划（Planning） | “我需要将任务拆成 3 步：1）收集数据，2）分析趋势，3）生成报告。” |
| 分析（Analysis） | “根据错误提示，问题似乎出在数据库连接参数上。” |
| 决策制定（Decision） | “考虑到用户预算限制，我应推荐中端选项。” |
| 问题解决（Problem Solving） | “要优化这段代码，需先做性能分析定位瓶颈。” |
| 记忆整合（Memory） | “用户之前提到偏好 Python，因此我会用 Python 示例。” |
| 自我反思（Reflection） | “上一种方法效果不佳，我应该尝试新策略。” |
| 目标设定（Goal） | “要完成此任务，需先确定验收标准。” |
| 优先级排序（Priority） | “加新功能前，应先解决安全漏洞。” |

在一些为“函数调用”微调过的模型里，Thought 甚至是 可选 的：
- 你可以要求它“直接给行动，不要写内心独白”，
- 也可以允许它展开思考，提升决策质量。

---

## 二、CoT：只用大模型“自己想清楚”的思维链

### 1. Chain-of-Thought 是什么？

Chain-of-Thought（CoT）= 思维链提示技术。

它的核心做法非常简单粗暴：

不要直接问模型“答案是什么”，
而是提示它：“Let’s think step by step（让我们一步一步来想）。”

也就是说，我们用 Prompt 把模型从“直接吐答案”切换成“边想边写”的模式。

### 2. CoT 的特点
- 只依赖 LLM 本身，不调用外部工具；
- 支持逐步推理，尤其适合：
  - 逻辑推理
  - 数学计算
  - 纯文本上的演绎/归纳

简单示例（来自课程中的经典 CoT 案例）：

问题：200 的 15% 是多少？
思维链：
- 200 的 10% 是 20；
- 200 的 5% 是 10；
- 所以 15% = 20 + 10 = 30。
答案：30。

如果不加 CoT 提示，模型可能直接写出“30”，看不出推导过程。
有了 CoT，它会把中间过程显式写出来，对可解释性和正确率都有帮助。

### 3. CoT 的局限

CoT 有一个天然的边界：

它只适合在模型“自己脑子里”就能解决的任务，
不适合需要实时数据 / 外部系统的场景。

比如：
- 查最新汇率？
- 搜索某个小众论文？
- 调用你公司内部的订单系统？

这些事情没法靠“想一想”解决，必须要“动手做事”——这就轮到 ReAct 出场了。

---

## 三、ReAct：让 Thought 和 Action 交替进行的智能体提示范式

### 1. ReAct 是什么？

ReAct = Reasoning + Acting
可以理解为：

在 CoT 的基础上，插入“工具调用”这一步，
通过「思考 → 行动 → 观察 → 再思考」这种循环解决复杂任务。

它和前一篇里说的 “Thought–Action–Observation 循环”是一脉相承的：
1. Thought（思考）：
   - 内部推理现在该干啥；
   - 决定是否要调用工具、调用哪个工具、带什么参数。
2. Action（行动）：
   - 按约定格式输出工具调用指令（例如 Search[...] / JSON）。
3. Observation（观察）：
   - 接收工具返回结果；
   - 把结果加入上下文，让下一轮 Thought 使用。
4. 重复这个循环，直到任务完成。

### 2. 看一个 ReAct 小例子：查巴黎天气

假设我们设计了两个动作：Search[...] & Finish[...]
用户问题：“现在巴黎天气怎么样？”

一次完整的 ReAct 轨迹可能是：

```text
Thought: 我需要查找巴黎最新的天气。
Action: Search["巴黎的天气"]

Observation: 搜索结果显示：气温为 18 摄氏度，多云。

Thought: 现在我已经知道巴黎的天气情况了，可以给出最终答案。
Action: Finish["巴黎的气温为 18 摄氏度，天气多云。"]
```

你会发现：
- Thought 负责解释“我为什么要这么查/要不要继续查”；
- Action 是真正和外部工具交互的地方；
- Observation 把新信息“喂回去”，让下一步思考更充分。

这就是一个典型的 ReAct 风格智能体过程。

### 3. ReAct 适合什么场景？

对比 CoT：
- CoT：适合“想清楚就够了”的任务，比如数学题、逻辑推理。
- ReAct：适合“边想边查”的任务，比如：
  - 信息搜索 / 价格对比；
  - 购物、订票、选酒店这类多步决策任务；
  - 读文档 + 调用 API + 写报告的长链路流程。

---

## 四、CoT vs ReAct：一句话总结 + 对照表

一句话对比：

CoT = 只在脑子里想清楚；
ReAct = 一边想一边动手做事。

表格版对比：

| 对比维度 | 思维链 CoT | ReAct 方法 |
| --- | --- | --- |
| 是否支持分步推理 | ✅ 支持 | ✅ 支持 |
| 是否使用外部工具 | ❌ 不使用，完全依赖 LLM 内部知识 | ✅ 强调工具调用（Action + Observation） |
| 典型场景 | 数学题、逻辑推理、纯文本问题 | 信息搜索、动态多步任务、需要实时数据的任务 |
| 对环境依赖 | 低（模型参数内知识即可） | 高（需要 APIs、数据库、系统等配合） |

一般的选型思路可以是：
- 如果你能用一句话描述成「这是一个算的问题」 → 优先考虑 CoT；
- 如果你能用一句话描述成「这是一个查 + 决策的问题」 → 优先考虑 ReAct。

---

## 五、新一代“会思考的模型”：推理-答案分离是训练级能力，不等于 CoT / ReAct

最近很多人会提到：
- DeepSeek R1 / R1-Zero
- OpenAI o1 系列

这类模型的一个共同特点是：

在生成最终答案前，会显式生成一段“推理过程”（ reasoning ），
并在输出格式上用特殊标记分离推理和答案。

例如内部可能有类似：

```xml
<reasoning>
（这里是一大段模型自己的推理过程）
</reasoning>
<answer>
（这里是给用户的最终答案）
</answer>
```

这里有两个容易混淆的点需要澄清：
1. 这些标签 + 分段，是训练阶段就教给模型的“行为模式”，属于 模型层面的能力；
2. CoT / ReAct 更多是“提示工程层面”的策略，是在“如何给模型下指令”。

可以粗暴地理解为：
- CoT / ReAct：
  - “我教你在这道题里，先想一想再回答。”（Prompt 策略）
- R1 / o1 这类：
  - “我在训练时就告诉你：你天生就要先推理再给答案。”（模型能力）

在工程实践里，两者是可以叠加的：
- 你可以对一个“会显式推理”的模型，继续用 ReAct 提示模式，
- 让它在“内部推理”与“工具行动”之间来回穿梭。

---

## 六、对工程实践的启发：写智能体时要想清楚这几件事

最后，用几个实践向的问题收个尾。如果你打算自己写一个 Agent，可以先想清楚：
1. 我需不需要显式 Thought？
   - 是希望它“直接调工具”，还是“先写清楚自己在想什么”？
   - 是否需要把 Thought 记录在日志里，方便调试与复盘？
2. 任务是更偏 CoT 还是更偏 ReAct？
   - 纯推理/数学/逻辑题：优先 CoT，简单省事；
   - 需要查 API / 搜索 / 访问数据库：优先 ReAct。
3. Action 和 Observation 的格式要不要统一？
   - 工具调用用什么协议？JSON？函数调用？特殊标记？
   - 工具返回怎么组织给模型看才不会“信息过载”？
4. 要不要为“推理-答案分离”做预留？
   - 是否采用支持 <reasoning> + <answer> 的模型？
   - 是否在前端 / 日志 / 审计系统中，区分“看到的答案”和“看不见的思考过程”？

---

## 总结
- Thought：是智能体的内部推理/内心独白，用来决定下一步动作。
- CoT：是只在模型脑子里“逐步思考”的提示方式，适合纯推理任务。
- ReAct：是在“思考”间隙插入“行动 + 观察”的范式，用来解决需要外部工具、多步搜索的复杂任务。
- 新一代 reasoning 模型（比如 DeepSeek R1 / OpenAI o1），通过训练让“先推理再回答”成为模型自带能力，可以与 CoT / ReAct 协同工作。

如果你已经在玩 RAG、工具调用、Agent 框架，
把这三层概念捋顺——Thought / CoT / ReAct——会让你在设计智能体时清晰很多：

什么时候让模型“多想想”，
什么时候让模型“先去查一查”，
什么时候该“想完就出手”。
