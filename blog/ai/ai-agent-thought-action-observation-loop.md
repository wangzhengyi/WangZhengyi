# 通过「思考-行动-观察」循环，重新理解 AI 智能体

如果你已经在用 ChatGPT、通义千问、豆包之类的大模型，你可能会发现：
模型本身很强，但一旦要“调用工具”“查实时数据”“拆解复杂任务”，就需要一个更完整的“智能体（Agent）”框架。

在很多智能体框架中（如 Hugging Face 的 smolagents、LangGraph 等），一个核心共识是：

AI 智能体 = 驱动 “思考 → 行动 → 观察” 循环的系统。

这篇文章就围绕这个循环，用工程师能看懂的方式，把“AI 智能体”这件事讲清楚：
	•	思考（Thought）：大模型到底在“想什么”？
	•	行动（Action）：如何调用工具、API、数据库？
	•	观察（Observation）：调用完之后，怎么利用结果继续推理？

---

## 一、从大模型到智能体：多了一层“会动手做事”的外壳

大语言模型（LLM）擅长的是：给定输入 → 直接生成输出，比如：
 - 提问题 → 回答
 - 给需求 → 出代码
 - 给一段文本 → 总结/翻译/润色

但在很多真实业务场景中，仅靠“纯文本推理”是不够的，比如：
 - 查询实时天气、汇率、股票价格（需要调用外部 API）；
 - 在公司内网检索文档、数据库（需要访问私有数据源）；
 - 执行多步任务（先查 A，再查 B，最后综合；或者失败后重试）。

这时就需要“AI 智能体（AI Agent）”：

简单理解：智能体 = 以 LLM 为大脑的“自动化小助手”，
能够自己思考下一步要做什么、调用什么工具、根据结果调整策略。

而让智能体从“能聊天”变成“会做事”的关键，就是下面这个循环：

Thought（思考） → Action（行动） → Observation（观察） → Thought（思考） → ...

你可以把它想成一个有记忆的 while 循环，每一轮都会用最新的观察结果来更新“下一步怎么干”。

---

## 二、Thought-Action-Observation：智能体的一次“呼吸”

从工程视角看，一个 AI 智能体在处理任务时，会不断做三件事：

1. Thought（思考）：用 LLM 决定“下一步做啥”

这一阶段发生在 LLM 内部推理：
 - 理解用户目标（例如：查询天气、订机票、调接口）；
 - 结合当前的上下文、已有观察结果，思考：
 - 我是否需要调用工具？
 - 如果需要，用哪个工具？
 - 要传哪些参数？

你可以把它类比成“人类脑子里打草稿”：
 - “用户问纽约天气，我自己不知道实时天气”
 - “我有一个 get_weather 工具，能查天气”
 - “那我应该调用 get_weather(‘New York’)”。

注意：这一阶段的输出不一定直接发给用户，而是为下一步的工具调用“写指令”。

---

2. Action（行动）：用“工具调用”去接触真实世界

有了 Thought 之后，智能体会进入 行动阶段，通常表现为：
 - 生成一个结构化的工具调用请求（例如 JSON）；
 - 指定要用的工具名 + 参数：

```json
{
  "action": "get_weather",
  "action_input": {
    "location": "New York"
  }
}
```

在工程实现上，这一步可以有多种形式：
 - HTTP 请求：调用 RESTful API；
 - 调用本地函数：如 tools.get_weather(location="New York")；
 - 访问数据库：如执行 SQL 查询；
 - 调用第三方 SDK 等。

本质：Action 是智能体与“环境/外部世界”互动的唯一出口。
它让 LLM 不再只是“文本黑盒”，而是能主动去“做事”。

---

3. Observation（观察）：拿到结果后，重新思考

工具执行完毕后，会返回一份结果，比如：

```json
{
  "location": "New York",
  "description": "partly cloudy",
  "temperature_c": 15,
  "humidity": 0.6
}
```

这一步叫 Observation（观察）：
 - 智能体把这段结果拼回上下文，喂给下一轮 LLM；
 - LLM 会在新的 Thought 中评估：
 - 数据是否完整？
 - 是否需要补充查询？
 - 是否发生错误（比如 404、参数错误）？
 - 现在是否已经足够回答用户？

如果一切正常，它可能得出一个新的 Thought：

“我已经得到了完整的天气数据，下一步可以给用户一个自然语言答案。”

这时就会回到 Thought 阶段，继续循环：
要么再发起新的 Action，要么直接输出给用户，结束循环。

---

## 三、用一个天气智能体例子，把循环走一遍

下面我们用一个简单的“天气智能体 Alfred”，看看完整流程如何运行。
用户问题：“纽约现在天气怎么样？”

1. 第一次 Thought：决定调用天气工具

LLM 内部推理大致是：

“用户在问纽约当前天气，我自己没有实时天气知识。
我有一个 get_weather 工具，可以查实时天气。
下一步应该调用 get_weather("New York")。”

这一阶段的可视化“心声”可以长这样（伪代码/伪思考）：

```
Thought:
User asked for the current weather in New York.
I should use the `get_weather` tool with location = "New York".
```

2. 第一次 Action：调用 get_weather

智能体根据 Thought 生成工具调用指令（工具调用协议因框架而异，这里用 JSON 举例）：

```json
{
  "action": "get_weather",
  "action_input": {
    "location": "New York"
  }
}
```

框架接管这条指令，实际去调用天气 API。

3. 第一次 Observation：拿到天气数据

假设 API 返回：

```json
{
  "location": "New York",
  "description": "partly cloudy",
  "temperature_c": 15,
  "humidity": 0.6
}
```

智能体把这段数据作为 Observation，拼回对话历史，交给 LLM。

4. Updated Thought：基于返回结果再思考

LLM 再次推理：

“我已经拿到了纽约的天气信息：局部多云，15℃，湿度 60%。
现在可以用中文整理成一段对用户友好的回答。”

此时的 Thought 更偏向“结果组织”，而不是再调用工具：

Thought:
I now know the weather in New York. 
I should respond in Chinese with a clear, friendly summary for the user.

5. Final Action：给用户最终答案

最终，智能体向用户输出自然语言：

“纽约当前天气为局部多云，气温约 15℃，湿度大约 60%。”

到这里，这一轮 Thought-Action-Observation 循环结束。

---

## 四、为什么一定要“循环”，而不是一次性完成？

你可能会问：
“我直接让模型写一句‘先查天气再回答’不行吗？为什么要搞循环？”

关键原因有三个：

1. 任务往往不止一步

真实场景经常是多步骤任务，比如：
 - 先查用户城市 → 再查城市天气 → 再换算单位 → 再生成报告；
 - 先登录 → 再查订单 → 再更新状态 → 再写回数据库。

如果用“单轮调用工具”的思路，每一种组合都要写死逻辑，扩展性差。
循环机制让智能体可以动态决定要走几步、每一步该干啥。

---

2. 需要根据“观察结果”实时纠错

比如天气 API 返回错误：
 - 参数错了（NewWork 拼错）
 - API 超时
 - 返回的是昨天的数据

如果没有 Observation + 下一轮 Thought：
 - 智能体完全意识不到错误；
 - 用户只能看到一条“看起来有点奇怪”的回答。

而有了 Thought-Action-Observation 循环：
 - 智能体可以判断“好像不对劲，再查一次/提醒用户补充信息”；
 - 提升了鲁棒性和自适应性。

---

3. 给智能体“可编排”的能力

从框架视角看，这个循环很像：

```python
while not done:
    thought = llm_think(history, tools)
    action = parse_action(thought)
    result = call_tool(action)
    history.append(result)
```

你可以在：
 - 循环前后加上各种 规则（系统提示、工具说明、任务约束）；
 - 中途插入 监控/日志/评估（比如记录每一步调用了什么工具、用了多久）；
 - 和 ReAct、LangGraph 等更复杂的控制流结合起来。

所以，思考-行动-观察循环本身，就是“可编排智能体”的基础控制结构。

---

## 五、工程视角：系统提示、工具列表和“安全栏杆”

在实际开发智能体时，有几个工程要点特别关键：

1. System Prompt：写清“规则”和“能力边界”

系统提示（System Prompt）里通常会写：
 - 智能体的角色与目标（你是谁、要帮用户做什么）；
 - 可用工具列表（名称、参数说明、适用场景）；
 - 思考-行动-观察的格式约定（比如 Thought 只做思考，不直接回答用户；Action 必须输出合法 JSON）。

这一块的作用是：
给智能体一套“工作手册”和“行为协议”。

---

2. 工具（Tools）：智能体的“手脚”

工具可以是：
 - HTTP API（天气、汇率、搜索等）；
 - 本地函数（如 run_python(code)、query_db(sql)）；
 - 内部服务（推荐系统、风控系统等）。

工具本身要做到：
 - 入参可控（类型明确、有校验）；
 - 出参结构化（方便 Observation 阶段被 LLM 解读）；
 - 具备基本的异常处理（失败时返回错误原因而不是直接崩）。

---

3. 观察（Observation）：不仅是“拿数据”，更是“写进记忆”

Observation 不仅是“工具结果”，还是 智能体的记忆更新点。
你可以在这一阶段做很多事情：
 - 为调试/监控记录日志（比如：工具调用次数、成功率、耗时）；
 - 对结果做预处理（过滤敏感信息、统一字段格式）；
 - 为下一轮 Thought 提供更友好的上下文（例如只保留关键字段，而不是把一整段 JSON 原样塞回去）。

在更高级的智能体实现里，甚至会专门做一层“可观测性 & 评估”（Observability & Evaluation），
用来分析：
 - 某类任务调用了多少轮工具？
 - 总是出错的是哪一个工具？
 - 哪些 Observation 是噪音，哪些对决策真正有用？

---

## 六、小结：把智能体理解成“带工具的 while True + LLM”

用一句工程化的总结：

AI 智能体 = 以 LLM 为大脑，
通过“思考（Thought）→ 行动（Action）→ 观察（Observation）”循环，
不断调用工具、利用环境反馈，迭代完成复杂任务的系统。

这个循环带来的核心价值是：
1. 支持复杂多步任务：不再局限于“一问一答”。
2. 借助工具突破静态知识：实时 API / 数据库 / 内网系统都能接入。
3. 通过环境反馈不断纠错和优化：结果更可靠、更贴近用户目标。
4. 为更复杂的 ReAct、Agentic RAG、LangGraph 等高级模式打基础。

如果你已经熟悉大语言模型、会写 Prompt，
下一步要把这些能力真正“接到业务上”，就绕不开这套循环。